library(ggplot2)
library("ggplot2")
install.packages("ggplot2")
library(ggplot2)
index <- d$beta.exposure < 0
data.frame(a=1:6,y=(-6:1))
d<-data.frame(a=1:6,y=1:6)
d<-data.frame(x=1:6,y=1:6)
d
d$x=-d$x
d
index <- d$x < 0
index
install.packages("ggplot2")
library(ggplot2)
exp(2)
exp(1)
e^2
2.7^2
# Load required library
library(stats)
# Create a sample dataset (replace this with your own data)
data <- data.frame(
"Feature1" = c(1, 2, 3, 4, 5),
"Feature2" = c(2, 4, 6, 8, 10),
"Feature3" = c(3, 6, 9, 12, 15)
)
# Perform PCA
pca_result <- prcomp(data, scale = TRUE)
# Print the PCA summary
print(pca_result)
# Plot the first two principal components
biplot(pca_result)
as.matrix(pca_result)
as.matrix(pca_result$rotation)
mm<-pca_result$rotation
mm
mm<-as.data.frmae(pca_result$rotation)
mm<-as.datafrmae(pca_result$rotation)
mm<-as.data.frame(pca_result$rotation)
plot(mm$PC1,mm$2)
plot(mm$PC1,mm$PC2)
fviz_pca_ind(pca_result,
col.ind = "cos2", # Color by the quality of representation
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
)
plot(mm$PC1,mm$PC2)
points(mm$PC1,mm$PC2, pch = 16, col = "blue")
text(mm$PC1,mm$PC2, labels = rownames(mm), pos = 3, offset = 0.5, col = "red")
data
data2 <- data.frame(
"Feature1" = c(1, 2, 3, 4, 5,3,9),
"Feature2" = c(2, 4, 6, 8, 10,6,18),
"Feature3" = c(3, 6, 9, 12, 15,9,27)
)
# Perform PCA
pca_result <- prcomp(data2, scale = TRUE)
mm<-as.data.frame(pca_result$rotation)
plot(mm$PC1,mm$PC2)
points(mm$PC1,mm$PC2, pch = 16, col = "blue")
text(mm$PC1,mm$PC2, labels = rownames(mm), pos = 3, offset = 0.5, col = "red")
setwd("~/Desktop/Pro_FN/Paper_IFNg/Figure5h")
load(/Volumes/users/homes/jlu/Desktop/Pro_Mine/Paper_Omid/Point5/data/Point5.RData)
load("/Volumes/users/homes/jlu/Desktop/Pro_Mine/Paper_Omid/Point5/data/Point5.RData")
immune.combined<-readRDS("/Volumes/users/homes/jlu/Desktop/Pro_Mine/Paper_Omid/Point5/data/immune.combined2.RawCounts4.RDS")
head(immune.combined2.matrix)
head(immune.combined2.matrix$TA.sc_ifnr)
colnames(immune.combined2.matrix)
row.names(immune.combined2.matrix)
colnames(immune.combined2.matrix)
head(data.ratio2_wide2_m)
head(data.ratio2_wide2_2)
head(data.ratio2_wide2)
head(data.ratio2)
head(data.ratio2$treatment)
table(data.ratio2$treatment)
head(data.ratios)
head(data.ratio)
table(data.ratio$sampleID)
?stat_compare_means
shiny::runApp('test')
P_value <- c（0.0001, 0.001, 0.006, 0.03, 0.095, 0.117, 0.234, 0.552, 0.751, 0.985）
P_value <- c(0.0001, 0.001, 0.006, 0.03, 0.095, 0.117, 0.234, 0.552, 0.751, 0.985)
p.adjust (P_values, method="bonferroni")
p.adjust (P_value, method="bonferroni")
p.adjust (P_value, method="fdr")
p.adjust (P_value, method="bh")
p.adjust (P_value, method="BH")
rank(P_value)
library(escape)
BiocManager::install("escape")
library(escape)
?enrichIt
library(SingleCellExperiment)
library(SeuratObject)
library(Seurat)
pbmc_small <- get("pbmc_small")
DefaultAssay(pbmc_small)
pbmc_small@assays
seq(1, 99, by=10)
sd(c(1,0,0,1))
sd(c(100,0,0,100))
tmp<-readRDS("/Volumes/science/Donertas/projects/Pro_Sidewalk/itp_chen/tmp.rds")
edit(tmp)
fix(tmp)
> tmp<-readRDS("/Volumes/science/Donertas/projects/Pro_Sidewalk/itp_chen/tmp.rds")
tmp<-readRDS("/Volumes/science/Donertas/projects/Pro_Sidewalk/itp_chen/tmp.rds")
fix(tmp)
edit(tmp)
all_col_names
seurat_merged_filtered_tcr_meta_3d_summary_bar<-readRDS("/Users/jlu/Desktop/Pro_Sidewalk/ITP_Chen/Figures/figures/seurat_merged_filtered_tcr_meta_3d_summary_bar.rds")
## convert df to matrix
library(tidyverse)
mat_A1<-seurat_merged_filtered_tcr_meta_3d_summary_bar[seurat_merged_filtered_tcr_meta_3d_summary_bar$gid=="A1",c("BV","BJ","sum_freq_bj")]
mat_A2<-seurat_merged_filtered_tcr_meta_3d_summary_bar[seurat_merged_filtered_tcr_meta_3d_summary_bar$gid=="A2",c("BV","BJ","sum_freq_bj")]
mat_B1<-seurat_merged_filtered_tcr_meta_3d_summary_bar[seurat_merged_filtered_tcr_meta_3d_summary_bar$gid=="B1",c("BV","BJ","sum_freq_bj")]
mat_B2<-seurat_merged_filtered_tcr_meta_3d_summary_bar[seurat_merged_filtered_tcr_meta_3d_summary_bar$gid=="B2",c("BV","BJ","sum_freq_bj")]
mat_D<-seurat_merged_filtered_tcr_meta_3d_summary_bar[seurat_merged_filtered_tcr_meta_3d_summary_bar$gid=="D",c("BV","BJ","sum_freq_bj")]
mat_A1 <- mat_A1 %>% pivot_wider(names_from = BJ, values_from = sum_freq_bj)
mat_A2 <- mat_A2 %>% pivot_wider(names_from = BJ, values_from = sum_freq_bj)
mat_B1 <- mat_B1 %>% pivot_wider(names_from = BJ, values_from = sum_freq_bj)
mat_B2 <- mat_B2 %>% pivot_wider(names_from = BJ, values_from = sum_freq_bj)
mat_D <- mat_D %>% pivot_wider(names_from = BJ, values_from = sum_freq_bj)
mat_A1<-as.matrix(mat_A1)
mat_A2<-as.matrix(mat_A2)
mat_B1<-as.matrix(mat_B1)
mat_B2<-as.matrix(mat_B2)
mat_D<-as.matrix(mat_D)
row.names(mat_D)<-mat_D[,1]
mat_D<-mat_D[,-1]
row.names(mat_B2)<-mat_B2[,1]
mat_B2<-mat_B2[,-1]
row.names(mat_B1)<-mat_B1[,1]
mat_B1<-mat_B1[,-1]
row.names(mat_A2)<-mat_A2[,1]
mat_A2<-mat_A2[,-1]
row.names(mat_A1)<-mat_A1[,1]
mat_A1<-mat_A1[,-1]
# Extract unique row names
all_row_names <- unique(c(rownames(mat_A1), rownames(mat_A2), rownames(mat_B1), rownames(mat_B2), rownames(mat_D)))
# Extract unique column names
all_col_names <- unique(c(colnames(mat_A1), colnames(mat_A2), colnames(mat_B1), colnames(mat_B2), colnames(mat_D)))
head(all_col_names)
all_row_names[order(all_row_names)]
custom_sort <- function(x) {
letters_part <- gsub("\\d", "", x)
numbers_part <- as.numeric(gsub("\\D", "", x))
list(letters_part, numbers_part)
}
all_row_names[order(sapply(all_row_names, custom_sort))]
gsub("\\d", "", "TRBV10-1")
gsub("\\d", "", "TRBV10")
gsub("\\D", "", "TRBV10")
gsub("\\D", "", "TRBV10-1")
my_string <- "78-9"
# Split the string by "-"
parts <- strsplit(my_string, "-")
# Extract the first part
first_part <- parts[[1]][1]
parts[[1]][1]
parts[[1]][2]
custom_sort <- function(x) {
parts <- strsplit(my_string, "-")
f <- parts[[1]][1]
s <- parts[[1]][2]
letters_part <- gsub("\\d", "", f)
numbers_1 <- as.numeric(gsub("\\D", "", f))
numbers_2 <- as.numeric(gsub("\\D", "", s))
list(letters_part, numbers_1, numbers_2)
}
all_row_names[order(sapply(all_row_names, custom_sort))]
custom_sort("TRBV6-9")
custom_sort <- function(x) {
parts <- strsplit(x, "-")
f <- parts[[1]][1]
s <- parts[[1]][2]
letters_part <- gsub("\\d", "", f)
numbers_1 <- as.numeric(gsub("\\D", "", f))
numbers_2 <- as.numeric(gsub("\\D", "", s))
list(letters_part, numbers_1, numbers_2)
}
custom_sort("TRBV6-9")
sapply(all_row_names, custom_sort)
order(sapply(all_row_names, custom_sort))
order_indices <- order(sapply(all_row_names, function(x) unlist(custom_sort(x))))
all_row_names[order_indices]
order(sapply(all_row_names, function(x) unlist(custom_sort(x))))
all_row_names
library(gtools)
# Your vector
vec <- c("TRBV4-9", "TRBC5-3", "TRBV5-3", "TRBV5-2")
# Use mixedsort to order the vector
mixedsort(vec)
mixedsort(all_row_names)
mixedsort(all_row_names,na.last=FALSE)
gsub(".","-",mixedsort(gsub("-",".",all_row_names),na.last=FALSE))
gsub("\\.","-",mixedsort(gsub("-",".",all_row_names),na.last=FALSE))
gsub("\\.","-",gtools::mixedsort(gsub("-",".",all_row_names),na.last=FALSE))
# order the vector
all_row_names<-gsub("\\.","-",gtools::mixedsort(gsub("-",".",all_row_names),na.last=FALSE))
all_col_names<-gsub("\\.","-",gtools::mixedsort(gsub("-",".",all_col_names),na.last=FALSE))
# Create empty matrices with unified row and column names
unified_mat_A1 <- matrix(NA, nrow = length(all_row_names), ncol = length(all_col_names), dimnames = list(all_row_names, all_col_names))
unified_mat_A2 <- matrix(NA, nrow = length(all_row_names), ncol = length(all_col_names), dimnames = list(all_row_names, all_col_names))
unified_mat_B1 <- matrix(NA, nrow = length(all_row_names), ncol = length(all_col_names), dimnames = list(all_row_names, all_col_names))
unified_mat_B2 <- matrix(NA, nrow = length(all_row_names), ncol = length(all_col_names), dimnames = list(all_row_names, all_col_names))
unified_mat_D <- matrix(NA, nrow = length(all_row_names), ncol = length(all_col_names), dimnames = list(all_row_names, all_col_names))
# Fill the unified matrices with values from the original matrices
unified_mat_A1[rownames(mat_A1), colnames(mat_A1)] <- mat_A1
unified_mat_A2[rownames(mat_A2), colnames(mat_A2)] <- mat_A2
unified_mat_B1[rownames(mat_B1), colnames(mat_B1)] <- mat_B1
unified_mat_B2[rownames(mat_B2), colnames(mat_B2)] <- mat_B2
unified_mat_D[rownames(mat_D), colnames(mat_D)] <- mat_D
## output to excel format
library(openxlsx)
# Create a workbook
wb <- createWorkbook()
# Add sheets to the workbook
addWorksheet(wb, "Sheet_A1")
addWorksheet(wb, "Sheet_A2")
addWorksheet(wb, "Sheet_B1")
addWorksheet(wb, "Sheet_B2")
addWorksheet(wb, "Sheet_D")
# Write matrices to corresponding sheets
writeData(wb, sheet = "Sheet_A1", unified_mat_A1, startCol = 1, startRow = 1, colNames = TRUE,rowNames = TRUE)
writeData(wb, sheet = "Sheet_A2", unified_mat_A2, startCol = 1, startRow = 1, colNames = TRUE,rowNames = TRUE)
writeData(wb, sheet = "Sheet_B1", unified_mat_B1, startCol = 1, startRow = 1, colNames = TRUE,rowNames = TRUE)
writeData(wb, sheet = "Sheet_B2", unified_mat_B2, startCol = 1, startRow = 1, colNames = TRUE,rowNames = TRUE)
writeData(wb, sheet = "Sheet_D", unified_mat_D, startCol = 1, startRow = 1, colNames = TRUE,rowNames = TRUE)
# Save the workbook to an Excel file
saveWorkbook(wb, file = "./Downloads/output.xlsx")
savehistory("~/Desktop/Pro_Sidewalk/ITP_Chen/scripts/itp_pip40.Rhistory")
savehistory("~/Desktop/Pro_Sidewalk/ITP_Chen/scripts/itp_pip41.Rhistory")
########
plot<-plot_ly(seurat_merged_filtered_tcr_meta_3d_summary[seurat_merged_filtered_tcr_meta_3d_summary$gid%in%c("A1","A2"),], x = ~BV, y = ~BD, z = ~BJ, color = ~gid, colors = c("red","blue"), size = ~sum_freq, type = "scatter3d", mode="markers", marker = list(symbol = 'circle', sizemode = 'diameter',opacity=0.5), sizes = c(5, 50))
# Load the necessary package
library(dplyr)
# Sample dataframe
data <- data.frame(
Sample1 = c(0.1, 0.2, 0.3, 0.1, 0.05, 0.15, 0.1),
Sample2 = c(0.15, 0.25, 0.2, 0.05, 0.1, 0.1, 0.15),
Sample3 = c(0.2, 0.1, 0.25, 0.1, 0.1, 0.15, 0.1),
Category = c("Positive", "Positive", "Negative", "Negative", "Uncertain", "Positive", "Negative")
)
# Display the sample dataframe
print(data)
# Aggregate the values by the category
aggregated_data <- data %>%
group_by(Category) %>%
summarise(across(starts_with("Sample"), sum))
# Display the aggregated dataframe
print(aggregated_data)
library(CellChat)
install.packages("CellChat")
devtools::install_github("jinworks/CellChat")
# Example data: Genes associated with enriched pathways
pathways <- c("Pathway_A", "Pathway_B", "Pathway_C")
genes <- list(
Pathway_A = c("Gene1", "Gene2", "Gene3"),
Pathway_B = c("Gene2", "Gene4", "Gene5"),
Pathway_C = c("Gene1", "Gene5", "Gene6")
)
# Convert the data into edge pairs (gene-pathway relationships)
edges <- do.call(rbind, lapply(names(genes), function(pathway) {
data.frame(Pathway = pathway, Gene = genes[[pathway]])
}))
# Show the edges (gene-pathway pairs)
print(edges)
data(pbmcs)
library(devtools)
install_github("guokai8/scGSVA")
library(fgsea)
?fgsea
fgsea
devtools::install_github("zji90/TSCAN")
install.packages('fastICA')
install.packages('fastICA')
devtools::install_github("zji90/TSCAN")
library(TSCAN)
# Load the required library
library(VennDiagram)
# Define the gene lists
genes_of_importance <- c("geneA", "geneB", "geneC", "geneD", "geneE") # Replace with actual list
significant_DEGs <- c("geneC", "geneD", "geneF", "geneG", "geneH")   # Replace with actual list
# Calculate the overlap
overlap <- intersect(genes_of_importance, significant_DEGs)
# Calculate the contingency table
all_genes <- union(genes_of_importance, significant_DEGs)
not_genes_of_importance <- setdiff(all_genes, genes_of_importance)
not_significant_DEGs <- setdiff(all_genes, significant_DEGs)
contingency_table <- matrix(
c(length(overlap),
length(genes_of_importance) - length(overlap),
length(significant_DEGs) - length(overlap),
length(not_genes_of_importance) - length(significant_DEGs) + length(overlap)),
nrow = 2,
byrow = TRUE
)
colnames(contingency_table) <- c("In_DEGs", "Not_in_DEGs")
rownames(contingency_table) <- c("In_Important_Features", "Not_in_Important_Features")
# Perform Fisher's exact test
fisher_result <- fisher.test(contingency_table)
# Print the test result
print(fisher_result)
# Create the Venn diagram
venn.plot <- draw.pairwise.venn(
area1 = length(genes_of_importance),
area2 = length(significant_DEGs),
cross.area = length(overlap),
category = c("Important Features", "Significant DEGs"),
fill = c("blue", "red"),
alpha = 0.5,
cat.cex = 1.5,
margin = 0.1
)
# Save the plot
png("venn_diagram.png")
grid.draw(venn.plot)
dev.off()
getwd()
library(data.table)
library(fgsea)
library(ggplot2)
data(examplePathways)
data(exampleRanks)
head(exampleRanks)
View(exampleRanks)
seq(0, 1, length = 11)
library(WGCNA)
install.packages("/Users/jlu/Downloads/ggplot2_3.4.2.tar.gz", repos = NULL, type = "source", dependencies = TRUE)
install_version("ggplot2", version = "3.4.2", repos = "http://cran.us.r-project.org")
library(remotes)
install_version("ggplot2", version = "3.4.2", repos = "http://cran.us.r-project.org")
# Load required package
library(entropy)
# Set seed for reproducibility
set.seed(123)
# Number of genes
n_genes <- 300
# Simulate NSen expression:
# Upregulated genes: low in NSen
up_expr_nsen <- rpois(n_genes, lambda = 5)
# Downregulated genes: high in NSen
down_expr_nsen <- rpois(n_genes, lambda = 100)
# Combine NSen expression
expr_nsen <- c(up_expr_nsen, down_expr_nsen)
# Simulate Sen expression:
# Upregulated genes now moderate
up_expr_sen <- rpois(n_genes, lambda = 50)
# Downregulated genes drop but still higher than up_expr_sen
down_expr_sen <- rpois(n_genes, lambda = 60)
# Combine Sen expression
expr_sen <- c(up_expr_sen, down_expr_sen)
# Define manual entropy function
calc_entropy <- function(x) {
p <- x / sum(x)
p <- p[p > 0]  # Remove zeros to avoid log(0)
-sum(p * log2(p))
}
# Calculate entropy
entropy_nsen <- calc_entropy(expr_nsen)
entropy_sen <- calc_entropy(expr_sen)
# Print results
cat("Entropy in NSen:", entropy_nsen, "\n")
cat("Entropy in Sen:", entropy_sen, "\n")
cat("Change in entropy (Sen - NSen):", entropy_sen - entropy_nsen, "\n")
head(down_expr_sen)
# Set seed for reproducibility
set.seed(123)
# Simulate NSen gene expression (e.g., 300 genes)
n_genes <- 300
NSen_expr <- runif(n_genes, min = 1, max = 100)  # random expression values
# Convert to proportions
NSen_prop <- NSen_expr / sum(NSen_expr)
# Now create Sen expression with different absolute scale but same proportions
scaling_factor <- 5
Sen_expr <- NSen_expr * scaling_factor
Sen_prop <- Sen_expr / sum(Sen_expr)
# Function to calculate entropy from proportions
entropy <- function(p) {
p <- p[p > 0]  # remove zeros to avoid log(0)
-sum(p * log(p))
}
# Calculate entropy
NSen_entropy <- entropy(NSen_prop)
Sen_entropy <- entropy(Sen_prop)
# Output
cat("Entropy NSen:", NSen_entropy, "\n")
cat("Entropy Sen :", Sen_entropy, "\n")
cat("Difference  :", Sen_entropy - NSen_entropy, "\n")
install.packages("ggplot2")
install.packages("ggplot2", dependencies = TRUE)
install.packages("ggplot2")
install.packages("tidyverse")
library(ggplot2)
install.packages("tidyverse")
install.packages("ggplot2")
library(ggplot2)
library(tidyverse)
packageurl <- "http://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_3.4.5.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
packageurl <- "http://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_3.4.4.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
packageurl <- "http://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_3.4.3.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
packageurl <- "http://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_3.3.3.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
install.packages("http://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_3.4.4.tar.gz", repos=NULL, type="source")
"https://storage.googleapis.com/ngdx-runs-archive/Raw_Data/Novaseq/211110_A00560_0212_AHWKCWDMXX/Reads/Metagenomica_Test/2111021704-NP-6_S53_L001_R1_001.fastq.gz?x-goog-signature=27e7fc1cd4f94522df123650a1315b274b2637df37b74cb87bcf8c5e4a54017856de10b8b33b5e947010ca9775acd69a1a252212b5f3f4fb398b7fa68fd16d3a2cd08e95482f0ef471498dd86f0195e6cafaed8dc7f3a7e7f9b30600b426828608812516c1d463d74fbd372856df1ade0764cccad60acbb1751d5fb06f141b1971077e664bf17d0934a2ad61387bf08257c08b77a1286c7a5599fcc9fb3ea2981b58ca363a73577c700f2bb091efe7b9fe040cc30d948ccf7f3710f1c1819eacc81ec1467bfa0f0e9a2a7fc811558e35e0653c1e46cff73e1a0f9226d4e943903366dcaf13d3b361f5ec864f2d80bb19df7fa14960b0382a2dca39910c823513&x-goog-algorithm=GOOG4-RSA-SHA256&x-goog-credential=signedurlsshare%40ngdx-cloud-resources.iam.gserviceaccount.com%2F20250707%2Feurope-west1%2Fstorage%2Fgoog4_request&x-goog-date=20250707T122803Z&x-goog-expires=604800&x-goog-signedheaders=host" == "https://storage.googleapis.com/ngdx-runs-archive/Raw_Data/Novaseq/211110_A00560_0212_AHWKCWDMXX/Reads/Metagenomica_Test/2111021704-NP-6_S53_L001_R1_001.fastq.gz?x-goog-signature=27e7fc1cd4f94522df123650a1315b274b2637df37b74cb87bcf8c5e4a54017856de10b8b33b5e947010ca9775acd69a1a252212b5f3f4fb398b7fa68fd16d3a2cd08e95482f0ef471498dd86f0195e6cafaed8dc7f3a7e7f9b30600b426828608812516c1d463d74fbd372856df1ade0764cccad60acbb1751d5fb06f141b1971077e664bf17d0934a2ad61387bf08257c08b77a1286c7a5599fcc9fb3ea2981b58ca363a73577c700f2bb091efe7b9fe040cc30d948ccf7f3710f1c1819eacc81ec1467bfa0f0e9a2a7fc811558e35e0653c1e46cff73e1a0f9226d4e943903366dcaf13d3b361f5ec864f2d80bb19df7fa14960b0382a2dca39910c823513&x-goog-algorithm=GOOG4-RSA-SHA256&x-goog-credential=signedurlsshare%40ngdx-cloud-resources.iam.gserviceaccount.com%2F20250707%2Feurope-west1%2Fstorage%2Fgoog4_request&x-goog-date=20250707T122803Z&x-goog-expires=604800&x-goog-signedheaders=host"
"https://storage.googleapis.com/ngdx-runs-archive/Raw_Data/Novaseq/230428_A00560_0306_AHWL22DRX2/Reads/80w_PiZ_F_M/202303280427_NP-21_S1_L002_R1_001.fastq.gz?x-goog-signature=9d5f54a9f25296d7639c191d841476b631091134284b92396e9bb5274da870f99b0d362eefc2a2c70967ec7a84ea98190222cb8fe8e50c1cf1388808f7514862637c144c2d25f6c6c3bf8e9bb55e4d29c6a6ccd108b413b141277134e24cad6d3b2dcbfc4a9d2bc5b8f9b003acb962e7658a5a1c69b4e45fa7fdaa1d7f0dc3f413d3eb4db4b92d32c19103d40d3fe5ccc650d278ac471b6997b82602da0bc6658313dc4bf9f07852888ed8aea5aa375024a15701b0e0f1c445d99d75c70cf9cb9aea6d359e3d724b03ae4000348eaa11b5ae3d4fa6336129a4d04f0dce8202991037e2c37b5089ea971917fa9bfeb9b9a8c097cb602bedb0356aa60adacc1f25&x-goog-algorithm=GOOG4-RSA-SHA256&x-goog-credential=signedurlsshare%40ngdx-cloud-resources.iam.gserviceaccount.com%2F20250707%2Feurope-west1%2Fstorage%2Fgoog4_request&x-goog-date=20250707T123907Z&x-goog-expires=604800&x-goog-signedheaders=host" == "https://storage.googleapis.com/ngdx-runs-archive/Raw_Data/Novaseq/230428_A00560_0306_AHWL22DRX2/Reads/80w_PiZ_F_M/202303280427_NP-21_S1_L002_R2_001.fastq.gz?x-goog-signature=624c9e2ceacef4a5c0a8a8a7502d20fc31b2f596d3b4d66d91f62207b08e4ac92f6ae38d17e0c0ddb0adfa7677bd4953c9ef126ff206a3ee7bf3ffb74d7ce1d4eb19f962af78aa3baf35e4b7df515c418530c495bc5f3eab1120cc67746f4ccc2324e22d42eec2dc3355abdb9ebeed516af3b1ac01f1ff834a33275c677c1959c9cc48d43e95b949cd455f5f83b1b4443a4c794fe220b3d326f8dbe4e356ebf8838eb55182d6eb9f3588e57a4e03f99cafce0730b0ce54b17bd8d35e4f782470d74c0b6cbc806a0ffdbbb5a4a7cc080a17626feb61afbfc4db4ac485cea0ce48ee2abb83e32d8f3ba785ae0be8017bba11cf0520dc7babb6135333f28ce218b7&x-goog-algorithm=GOOG4-RSA-SHA256&x-goog-credential=signedurlsshare%40ngdx-cloud-resources.iam.gserviceaccount.com%2F20250707%2Feurope-west1%2Fstorage%2Fgoog4_request&x-goog-date=20250707T123923Z&x-goog-expires=604800&x-goog-signedheaders=host"
"https://storage.googleapis.com/ngdx-runs-archive/Raw_Data/Novaseq/221207_A00560_0279_AHKLMKDRX2/Reads/PiZ_6W_F/202211240913_NP-20_S27_L002_R1_001.fastq.gz?x-goog-signature=037b0c6bf7b76011c150a5cb26cf4875fe4f206b45e4fb1a0eb38ddf20e95feb457498531d3c15e28fc700c99b21ed2f14cf47b6a4c19f6c7a409eea840233c9a76e50e94944c3b22f63805af75388f2b82597331f8b8b036ba7e5212efd9d4d0344beb7cfd0bb7c4c7fcc10b363496c9127fc1396d6f67810e5c2daa198d50492d66884b544e58a8839c37568ed6bc50834e2fa4297983568f7d5015ded8ad6fbdf5abe1fc33fce66a21c71e5cac2dbba53bea89af9cfc5da30de47a65d53b56595ae51350a80fc3045d8d9d6143b94f33c23c9e27e3f9c89f7bd3e8b692da32a1f7abb7b68a3f8e3a8a3db1ec291a6ce69edccc2cce736fe0c6af551ad22e2&x-goog-algorithm=GOOG4-RSA-SHA256&x-goog-credential=signedurlsshare%40ngdx-cloud-resources.iam.gserviceaccount.com%2F20250707%2Feurope-west1%2Fstorage%2Fgoog4_request&x-goog-date=20250707T124552Z&x-goog-expires=604800&x-goog-signedheaders=host" == "https://storage.googleapis.com/ngdx-runs-archive/Raw_Data/Novaseq/221207_A00560_0279_AHKLMKDRX2/Reads/PiZ_6W_F/202211240913_NP-20_S27_L002_R2_001.fastq.gz?x-goog-signature=a439536bfc81540721a41605547e25b5f34ac73a18a249f07ef699c1c8dfbf895613742a380b4451c67cf944d50aa7a349c2d5cecf58012128804150dc2b353937b9fe14b1b342939f13d80885a7f8b657c7d54eac50606ed35d704f5cebddd8214befc5f94a80a59f05f1145a8b44ec93bf6712b8db30827ac9a5efa2d83210d0b4c3d6ed11bb86919bd467cc61c9bb29b8c4407d3e2b2c855ec7053c494d414c5a05e083077be6efad42719161d125493a7be1a795cdc71b54e959ea8cda97a8382e3e90d5ed6befe4cf17c3fc23f576c16186b65a36321cdc89f05146250d881ff5459cf040144df8bdf66bb32a6ef4849d29ba6fc877ec3b914f78f49938&x-goog-algorithm=GOOG4-RSA-SHA256&x-goog-credential=signedurlsshare%40ngdx-cloud-resources.iam.gserviceaccount.com%2F20250707%2Feurope-west1%2Fstorage%2Fgoog4_request&x-goog-date=20250707T124535Z&x-goog-expires=604800&x-goog-signedheaders=host"
"https://storage.googleapis.com/ngdx-runs-archive/Raw_Data/Novaseq/211223_A00560_0226_AH2K5VDSX3/Reads/meta_36w/202112211031_NP-9_S21_L004_R1_001.fastq.gz?x-goog-signature=47696e88ee9f1def53b7ae2d12acbf30edaabc21a740acdda5a77936ebc9621b10deae2dce621047aa360118ae2d78023452fdeaffbeabb17fbed3a288acbb302925ccd8a9c288874ec7ea310c837dbd8caaa2dff8edcb71295d3e0dccc7ad05f874bb89eb41e66082cf66af96cd7f6ad931886c1c40d10d13b43cf783acac36a71bd834a361bded6724ea851c0bc5c9086eca3c43e5e42ef9534cbdb09b262d992717d99aca215157546cda841846796b1311a29665fabd6fd7bc09093153b19e2c8044a716532814e651d1b0bdcad06306563496af0631470839371e81c559fa7fa1c63ce51c80351364ad9fca82b6a00a9d2b77a72cc31f25183f401f11a1&x-goog-algorithm=GOOG4-RSA-SHA256&x-goog-credential=signedurlsshare%40ngdx-cloud-resources.iam.gserviceaccount.com%2F20250707%2Feurope-west1%2Fstorage%2Fgoog4_request&x-goog-date=20250707T123437Z&x-goog-expires=604800&x-goog-signedheaders=host" == "https://storage.googleapis.com/ngdx-runs-archive/Raw_Data/Novaseq/211223_A00560_0226_AH2K5VDSX3/Reads/meta_36w/202112211031_NP-9_S21_L004_R2_001.fastq.gz?x-goog-signature=8be71fdeb3dc160767826b541c285e7eb76e3154bc17ec4dbe57ced085ad4090569d445e0119386dfdbf009bda41a769a19082940a51c4c2baf50d1b4a5f07434b4123eeebfa7208fdd10606abb5b50be00bf845bbe917f889ff3e3cd725ec57bdad84ab8ff205b32e0ebc7eda5174f99a6be9cbe1eb463ba4667b2ce6fa30fba67261ca6d48d94cd7582fd86c985eac960993edf56b02304a3d255871e952e69dccd2ff2a9e1bd2bddb6ebb09787f90568839d5aca63d67543a8871d747f770e5befe3c0c99e46c7898fc56aa87a2e8dd25d784e42f41ac1a1afa1860d14db457e57e0d1cb3467ab0f82405a7dde14576a89cbdbbaeae1b04af4752dd276195&x-goog-algorithm=GOOG4-RSA-SHA256&x-goog-credential=signedurlsshare%40ngdx-cloud-resources.iam.gserviceaccount.com%2F20250707%2Feurope-west1%2Fstorage%2Fgoog4_request&x-goog-date=20250707T123557Z&x-goog-expires=604800&x-goog-signedheaders=host"
"https://storage.googleapis.com/ngdx-runs-archive/Raw_Data/Novaseq/211110_A00560_0212_AHWKCWDMXX/Reads/Metagenomica_Test/2111021704-NP-9_S56_L001_R1_001.fastq.gz?x-goog-signature=86d3a04c0c285cdb3a08e26a966a906a8e6dfc47f715e7c4058d82e2b787cc92e85c0a412508ac7dd3a9a7d3cf40a41a1725b9445ab21a17cb6782416c4d3f9d0dad7a59837e3adb6061cb51414ecebdc8409241982215d4dd3738f6e47589aa473ac1875318e665c9e68d9927fc80705238b0d302a7974a2e7542d1a57b00bd1e85016f34b2de49f37c1943230594b00dfec3dca5b01811b503b9ef7b41250538dc396fdc6a3e6053acbc315719f33bd4afe71a7ba2f5edb6704f2d023572b3cbe2bb6c99df55ce2f83218eae98c16551be011c9e9e2c5ba3592867ce51eb96539f0699e40cb8309fd64b4c800715f2e4dba5a17b323a2a4b1d51340c236a33&x-goog-algorithm=GOOG4-RSA-SHA256&x-goog-credential=signedurlsshare%40ngdx-cloud-resources.iam.gserviceaccount.com%2F20250707%2Feurope-west1%2Fstorage%2Fgoog4_request&x-goog-date=20250707T122804Z&x-goog-expires=604800&x-goog-signedheaders=host" == "https://storage.googleapis.com/ngdx-runs-archive/Raw_Data/Novaseq/211110_A00560_0212_AHWKCWDMXX/Reads/Metagenomica_Test/2111021704-NP-9_S56_L001_R1_001.fastq.gz?x-goog-signature=86d3a04c0c285cdb3a08e26a966a906a8e6dfc47f715e7c4058d82e2b787cc92e85c0a412508ac7dd3a9a7d3cf40a41a1725b9445ab21a17cb6782416c4d3f9d0dad7a59837e3adb6061cb51414ecebdc8409241982215d4dd3738f6e47589aa473ac1875318e665c9e68d9927fc80705238b0d302a7974a2e7542d1a57b00bd1e85016f34b2de49f37c1943230594b00dfec3dca5b01811b503b9ef7b41250538dc396fdc6a3e6053acbc315719f33bd4afe71a7ba2f5edb6704f2d023572b3cbe2bb6c99df55ce2f83218eae98c16551be011c9e9e2c5ba3592867ce51eb96539f0699e40cb8309fd64b4c800715f2e4dba5a17b323a2a4b1d51340c236a33&x-goog-algorithm=GOOG4-RSA-SHA256&x-goog-credential=signedurlsshare%40ngdx-cloud-resources.iam.gserviceaccount.com%2F20250707%2Feurope-west1%2Fstorage%2Fgoog4_request&x-goog-date=20250707T122804Z&x-goog-expires=604800&x-goog-signedheaders=host"
"https://storage.googleapis.com/ngdx-runs-archive/Raw_Data/Novaseq/211110_A00560_0212_AHWKCWDMXX/Reads/Metagenomica_Test/2111021704-NP-5_S52_L001_R1_001.fastq.gz?x-goog-signature=169d6f9cd0592442ef4b523d3be5315f48b2b731886cc065b9e75c1d5d90c375a0650b74216439833f5db11eaab916a69e44e8ff10e017366d87459d9340aa36310c17f5fea380e046eed840340cfb3861f491b57e0f99914798f6afc7d7b72b3ca200ab6a1e306c5867bd349ff203ecc29641cec6ae05cbf47b7c632571dcdaddb81028704a44101125952c170ee1fd1bb4c92639a3ecf4bb874cdbf8a705e5da4e9dbaf007e1c325b8907ea969f6802ee2b30047654ff97e48e3d0411bbf0c9d87df7c4e2447697bfb268de53cd4c79576d045415a936b67d5fd76d7e4f8543f058db969c442ba3b7e55f394c1acab0766e2ef1248a811a3bccd712605b3e0&x-goog-algorithm=GOOG4-RSA-SHA256&x-goog-credential=signedurlsshare%40ngdx-cloud-resources.iam.gserviceaccount.com%2F20250707%2Feurope-west1%2Fstorage%2Fgoog4_request&x-goog-date=20250707T122803Z&x-goog-expires=604800&x-goog-signedheaders=host" == "https://storage.googleapis.com/ngdx-runs-archive/Raw_Data/Novaseq/211110_A00560_0212_AHWKCWDMXX/Reads/Metagenomica_Test/2111021704-NP-5_S52_L001_R1_001.fastq.gz?x-goog-signature=169d6f9cd0592442ef4b523d3be5315f48b2b731886cc065b9e75c1d5d90c375a0650b74216439833f5db11eaab916a69e44e8ff10e017366d87459d9340aa36310c17f5fea380e046eed840340cfb3861f491b57e0f99914798f6afc7d7b72b3ca200ab6a1e306c5867bd349ff203ecc29641cec6ae05cbf47b7c632571dcdaddb81028704a44101125952c170ee1fd1bb4c92639a3ecf4bb874cdbf8a705e5da4e9dbaf007e1c325b8907ea969f6802ee2b30047654ff97e48e3d0411bbf0c9d87df7c4e2447697bfb268de53cd4c79576d045415a936b67d5fd76d7e4f8543f058db969c442ba3b7e55f394c1acab0766e2ef1248a811a3bccd712605b3e0&x-goog-algorithm=GOOG4-RSA-SHA256&x-goog-credential=signedurlsshare%40ngdx-cloud-resources.iam.gserviceaccount.com%2F20250707%2Feurope-west1%2Fstorage%2Fgoog4_request&x-goog-date=20250707T122803Z&x-goog-expires=604800&x-goog-signedheaders=host"
"https://storage.googleapis.com/ngdx-runs-archive/Raw_Data/Novaseq/211110_A00560_0212_AHWKCWDMXX/Reads/Metagenomica_Test/2111021704-NP-10_S57_L001_R1_001.fastq.gz?x-goog-signature=39c9d01d0df5c5682d1928764b705f0f77f3aea75575f264664c63378cbd06a56766c9bd1532f549d320edc6cb0478eddebaf452f3ebb1175cd1720d9dfbbf2f4a11604eb8fb7f139fa6bd9d025116ca2b1aa5ed06145ab3975c55f39ca6616846c8eec53cc7e399c042211603896a8c5ac8291750afa02efe02a675c3b4a54e29061501ade3b36b083ae52b12436c71e1a451cd3a54135ab4fc69eb22ca30f52fcebc79b082114b95e8803dc0e2ebce5b185ec2fae87e307f9062a28aa458097dbd0d697a86ff8f4c918eb9bf25d1ae1beec67c297428e6f9c75373a38c15964ab72f548b5a49c2af4ff013dc8aea8f9d93811227e86907df183bbcf3bb9e50&x-goog-algorithm=GOOG4-RSA-SHA256&x-goog-credential=signedurlsshare%40ngdx-cloud-resources.iam.gserviceaccount.com%2F20250707%2Feurope-west1%2Fstorage%2Fgoog4_request&x-goog-date=20250707T122802Z&x-goog-expires=604800&x-goog-signedheaders=host" == "https://storage.googleapis.com/ngdx-runs-archive/Raw_Data/Novaseq/211110_A00560_0212_AHWKCWDMXX/Reads/Metagenomica_Test/2111021704-NP-10_S57_L001_R2_001.fastq.gz?x-goog-signature=7a2ae3e4df444bcba765ce9e5db07f507bd44c4a949ccf6456c91f5bebb34fb057c712a03d3df2fee9ce0334405629642ee77414c8fd68fef85dac55f39e5764e4427b7ea984fce6d1f3f58b60e6b5b573a5aa92fe8392cfdbfc442889abf9cd55bdefafb05d88f7a2ec5382c2f6c6d4f876c17ac05a31d22e8c2f3a1b62e1e5276456add543e86df52f4948c611fa507849452a975dc015dabb493e5fdaf38a58ba1beca1d47869351259fe0c6ff3d5e577e5153ba9345e033834765b223135fe63c89dee62e7c34509f1a32594bbdd13e330064629d3fb79ab74159c1701cb6f988d430462e5517260cd5349949100ee7bb92efce1f7159efbcbcc6308fa14&x-goog-algorithm=GOOG4-RSA-SHA256&x-goog-credential=signedurlsshare%40ngdx-cloud-resources.iam.gserviceaccount.com%2F20250709%2Feurope-west1%2Fstorage%2Fgoog4_request&x-goog-date=20250709T164453Z&x-goog-expires=604800&x-goog-signedheaders=host"
for (i in 1:50) {print(i)}
for (i in seq(1,50,by=1)) {print(i)}
# Create a sample test dataframe
test_df <- data.frame(
ID = 1:5,
Name = c("Alice", "Bob", "Charlie", "David", "Eva"),
Age = c(24, 30, 28, 35, 22),
Score = c(88, 92, 75, 85, 90)
)
# Print the dataframe
print(test_df)
row.names(test_df)
row.names(test_df)[1:1]
R.version.string
ls()
install.packages("ggplot2")
ls()
require(ISLR)
library(glmnet)
library(elasticnet)
x=model.matrix(Apps~.,College)[,-1]
install.packages("ISLR")
x=model.matrix(Apps~.,College)[,-1]
require(ISLR)
library(glmnet)
library(elasticnet)
x=model.matrix(Apps~.,College)[,-1]
y=College$Apps
enet.model=enet(x, y, lambda=1)
mx_x <- data.frame(coef_enet = round(predict(enet.model, type="coef", s=2, mode="penalty", naive = F)$coefficients, 2))
mx_x$coef_enet.naive <- round(predict(enet.model, type="coef", s=2, mode="penalty", naive = T)$coefficients, 2)
glmnet.model=glmnet(x, y, alpha=0.50)
mx_x$coef_glmnet <- round(coef(glmnet.model, s=2, exact=TRUE), 2)[-1]
mx_x
library(Seurat)
# 1. Create a sample dataset
set.seed(42)
# Create a matrix with 10 rows and 3 columns (variables)
original_data <- matrix(rnorm(30, mean = 10, sd = 5), ncol = 3)
colnames(original_data) <- c("Var1", "Var2", "Var3")
cat("--- Original Data Column Statistics ---\n")
# Check the mean and standard deviation of the original data columns
print(apply(original_data, 2, function(x) c(Mean = mean(x), SD = sd(x))))
# ---
# 2. Apply the scale function (default: center = TRUE, scale = TRUE)
scaled_data <- scale(original_data)
cat("\n--- Scaled Data Column Statistics ---\n")
# Check the mean and standard deviation of the scaled data columns
# The apply function with MARGIN=2 (columns) is used to check the results
scaled_stats <- apply(scaled_data, 2, function(x) c(Mean = mean(x), SD = sd(x)))
print(scaled_stats)
# ---
# 3. Explicitly verify the values
# Check if the means are close to 0 (due to floating point arithmetic)
mean_check <- all(abs(scaled_stats["Mean", ]) < 1e-15)
# Check if the standard deviations are close to 1
sd_check <- all(abs(scaled_stats["SD", ] - 1) < 1e-15)
cat("\n--- Verification ---\n")
cat(paste("Are all column means close to 0? ", mean_check, "\n"))
cat(paste("Are all column SDs close to 1? ", sd_check, "\n"))
# Load the necessary package
# install.packages("glmnet")
library(glmnet)
# --- 1. Data Simulation ---
set.seed(42)
N <- 100 # Samples
P <- 5   # Features
# Generate feature matrix X
X <- matrix(rnorm(N * P, mean = 5, sd = 2), ncol = P)
# Generate target y (linear relationship + noise)
y <- X %*% c(2, -1, 0.5, 0, 1) + rnorm(N)
# --- A. Model with DEFAULT Standardization ---
# The default argument is standardize = TRUE
# We use alpha = 0 (Ridge) and lambda = 0.1 (penalty strength) for simplicity.
model_standardized <- glmnet(
x = X,
y = y,
family = "gaussian",
alpha = 0, # Ridge Regression
lambda = 0.1 # Specific penalty
)
cat("--- Coefficients (Standardized Data) ---\n")
# The coef() function returns the standardized coefficients
# (and the intercept on the original scale)
coef_std <- coef(model_standardized, s = 0.1)
print(coef_std)
# ---
# --- B. Model WITHOUT Standardization ---
# Explicitly set standardize = FALSE
model_non_standardized <- glmnet(
x = X,
y = y,
family = "gaussian",
alpha = 0, # Ridge Regression
lambda = 0.1, # Specific penalty
standardize = FALSE # IMPORTANT: Turn off standardization
)
cat("\n--- Coefficients (Non-Standardized Data) ---\n")
# The coef() function returns the non-standardized coefficients
coef_non_std <- coef(model_non_standardized, s = 0.1)
print(coef_non_std)
# --- C. Base R GLM (No Standardization Applied) ---
# Note: glm does NOT have 'standardize' option, it works on raw data
model_base_glm <- glm(
y ~ X, # Use the original data
family = "gaussian"
)
cat("\n--- Coefficients (Base R glm) ---\n")
# The coef() function returns the non-standardized coefficients
print(coef(model_base_glm))
# Load the necessary package
# install.packages("glmnet")
library(glmnet)
# --- 1. Data Simulation ---
set.seed(42)
N <- 100 # Samples
P <- 5   # Features
# Generate feature matrix X
X <- matrix(rnorm(N * P, mean = 5, sd = 2), ncol = P)
# Generate target y (linear relationship + noise)
y <- X %*% c(2, -1, 0.5, 0, 1) + rnorm(N)
# --- A. Model with DEFAULT Standardization ---
# The default argument is standardize = TRUE
# We use alpha = 0 (Ridge) and lambda = 0.1 (penalty strength) for simplicity.
model_standardized <- glmnet(
x = X,
y = y,
family = "gaussian",
alpha = 0.5, # Ridge Regression
lambda = 0.1 # Specific penalty
)
cat("--- Coefficients (Standardized Data) ---\n")
# The coef() function returns the standardized coefficients
# (and the intercept on the original scale)
coef_std <- coef(model_standardized, s = 0.1)
print(coef_std)
# ---
# --- B. Model WITHOUT Standardization ---
# Explicitly set standardize = FALSE
model_non_standardized <- glmnet(
x = X,
y = y,
family = "gaussian",
alpha = 0.5, # Ridge Regression
lambda = 0.1, # Specific penalty
standardize = FALSE # IMPORTANT: Turn off standardization
)
cat("\n--- Coefficients (Non-Standardized Data) ---\n")
# The coef() function returns the non-standardized coefficients
coef_non_std <- coef(model_non_standardized, s = 0.1)
print(coef_non_std)
# Load the necessary package for penalized regression
# install.packages("glmnet")
library(glmnet)
library(ggplot2) # Used for plotting/data manipulation if desired
# Set the SAME seed as the Python example to ensure identical data generation
set.seed(42)
N <- 1000  # Number of samples
noise_sd <- 8
# 1. --- Data Simulation ---
# Continuous features (non-zero mean, different scales)
X1 <- rnorm(N, mean = 10, sd = 3)
X2 <- rnorm(N, mean = 50, sd = 10)
X3 <- runif(N, min = 0, max = 10)
# Categorical feature (Binary 0/1)
X_cat <- sample(c(0, 1), size = N, replace = TRUE, prob = c(0.6, 0.4))
# True Coefficients
beta1 <- 1.5
beta2 <- -0.2
beta3 <- 0.8
beta_cat <- 5.0
# Derived Terms for the True Model
X_nonlin <- X1^2 # Non-linear term
beta_nonlin <- -0.05
X_interact <- X2 * X_cat # Interaction term
beta_interact <- 0.1
# Generate Target Variable (y)
error <- rnorm(N, mean = 0, sd = noise_sd)
y <- (beta1 * X1 +
beta2 * X2 +
beta3 * X3 +
beta_cat * X_cat +
beta_nonlin * X_nonlin +
beta_interact * X_interact +
error)
# 2. --- Prepare the Final Design Matrix ---
# Combine all features (raw and derived) into a matrix
X_matrix <- as.matrix(data.frame(
cont_X1 = X1,
cont_X2 = X2,
cont_X3 = X3,
cat_X_bin = X_cat,
X1_sq = X_nonlin,
X2_by_cat = X_interact
))
# 3. --- Train/Test Split ---
# 70% Training / 30% Testing split
train_indices <- sample(1:N, size = round(0.7 * N))
X_train <- X_matrix[train_indices, ]
X_test <- X_matrix[-train_indices, ]
y_train <- y[train_indices]
y_test <- y[-train_indices]
cat("--- Data Split Check ---\n")
cat(paste("Training samples:", nrow(X_train), "\n"))
cat(paste("Test samples:", nrow(X_test), "\n"))
# 4. --- Elastic Net Model Fitting ---
# Parameters to match Python's sklearn example:
# alpha = 0.5      (L1/L2 mix, equivalent to l1_ratio=0.5 in Python)
# lambda = 0.1     (Penalty strength, equivalent to alpha=0.1 in Python)
# Fit the Elastic Net model
en_model <- glmnet(
x = X_train,
y = y_train,
alpha = 0.5,
lambda = 0.1,
family = "gaussian",
standardize = FALSE # Crucial for matching the Python default on raw data
)
# 5. --- Coefficient Extraction ---
# Extract coefficients for the specified lambda (0.1)
r_coefficients_full <- coef(en_model, s = 0.1)
cat("\n--- R Elastic Net Coefficients (Non-Standardized Fit) ---\n")
print(r_coefficients_full)
# Create a data frame for cleaner output
coefficient_names <- rownames(r_coefficients_full)
coefficient_values <- as.vector(r_coefficients_full)
coeff_df_r <- data.frame(
Feature = coefficient_names,
Coefficient = coefficient_values
)
cat("\n--- Cleaned Coefficient Table ---\n")
print(coeff_df_r)
# 6. --- Model with Standardization (for comparison) ---
# To see the effect of standardization (the glmnet default for data analysis)
en_model_std <- glmnet(
x = X_train,
y = y_train,
alpha = 0.5,
lambda = 0.1,
family = "gaussian",
standardize = TRUE # Default behavior (Standardizes features before fitting)
)
r_coefficients_std <- coef(en_model_std, s = 0.1)
cat("\n--- R Elastic Net Coefficients (Standardized Fit) ---\n")
# Note that these coefficients will be different due to the change in scale
print(r_coefficients_std)
# Load the necessary package
library(glmnet)
# Set the SAME seed
set.seed(42)
N <- 1000  # Number of samples
noise_sd <- 0.5 # Noise is controlled by the Poisson distribution's nature,
# but we'll use a small error term on the log link
# 1. --- Feature Generation ---
# Continuous feature 1: Time (positive skew)
X_Time <- rgamma(N, shape = 2, rate = 0.5)
# Continuous feature 2: Concentration (large scale)
X_Conc <- runif(N, min = 100, max = 500)
# Multi-level Categorical feature (Factor with 4 levels)
X_Factor <- factor(sample(c("A", "B", "C", "D"), size = N, replace = TRUE, prob = c(0.4, 0.3, 0.2, 0.1)))
# 2. --- Define the True Model and Target (Poisson Link) ---
# Define coefficients for the linear predictor (log(lambda))
# Intercept is the base log-rate
beta_Intercept <- 1.5
beta_Time <- 0.2
beta_Conc <- -0.005 # Small effect due to large X scale
# Categorical effects (relative to base level 'A')
beta_B <- 0.8
beta_C <- 1.2
beta_D <- 0.3
# Interaction Term: Concentration * Time
beta_Interaction <- -0.001
# 3. --- Generate Log-Linear Predictor (eta) ---
# Create design matrix components for the log-linear predictor (eta)
# R will automatically handle the dummy coding for X_Factor in the matrix creation
design_frame <- data.frame(
Time = X_Time,
Conc = X_Conc,
Factor = X_Factor
)
# Use model.matrix to create the design matrix with dummy variables and interaction
# This is how glmnet expects the categorical data
X_matrix <- model.matrix(~ Time * Conc + Factor, data = design_frame)[, -1] # Remove the original intercept
# Calculate the log-rate (eta) = X * Beta
# We use a placeholder for the X_matrix columns (Time, Conc, FactorB, FactorC, FactorD, Time:Conc)
# Note: The true vector must match the order of columns in X_matrix!
true_beta_vector <- c(
beta_Time,      # Time
beta_Conc,      # Conc
beta_B,         # FactorB
beta_C,         # FactorC
beta_D,         # FactorD
beta_Interaction # Time:Conc (Interaction)
)
# Calculate the linear predictor (eta)
eta <- beta_Intercept + X_matrix %*% true_beta_vector + rnorm(N, mean=0, sd=noise_sd)
# 4. --- Generate Poisson Count Outcome (y) ---
# lambda = exp(eta)
lambda <- exp(eta)
y_count <- rpois(N, lambda = lambda)
# 5. --- Train/Test Split ---
train_indices <- sample(1:N, size = round(0.7 * N))
X_train <- X_matrix[train_indices, ]
X_test <- X_matrix[-train_indices, ]
y_train <- y_count[train_indices]
y_test <- y_count[-train_indices]
# 6. --- Elastic Net Model Fitting (Poisson) ---
# Fit the Elastic Net Poisson GLM
en_model_poisson <- glmnet(
x = X_train,
y = y_train,
alpha = 0.5,      # Elastic Net mix
lambda = 0.05,    # Specific penalty (chosen smaller for more complex model)
family = "poisson", # Crucial change for count data
standardize = FALSE # Keeps coefficients on the raw feature scale
)
# 7. --- Coefficient Extraction and Display ---
r_coefficients_full <- coef(en_model_poisson, s = 0.05)
cat("\n--- R Elastic Net Coefficients (Poisson GLM) ---\n")
print(r_coefficients_full)
# Create a data frame for cleaner output
coefficient_names <- rownames(r_coefficients_full)
coefficient_values <- as.vector(r_coefficients_full)
coeff_df_r_poisson <- data.frame(
Feature = coefficient_names,
Coefficient = coefficient_values
)
cat("\n--- Cleaned Coefficient Table (Poisson GLM) ---\n")
print(coeff_df_r_poisson)
# install.packages("alr4")
# install.packages("glmnet")
library(alr4)
install.packages("alr4")
# install.packages("alr4")
# install.packages("glmnet")
library(alr4)
library(glmnet)
# Load the dataset (it is a data frame)
data(AIS)
# The model will predict Red Blood Cell count (RBC)
# using Weight, Height, and the categorical variable Sport,
# with an interaction between Weight and Sex.
# 1. Prepare Design Matrix (X) and Response (y)
# Use model.matrix to handle the categorical 'Sport' and the interaction term 'Wt:Sex'
# This creates dummy variables and the interaction column automatically.
X_matrix <- model.matrix(RBC ~ Wt * Sex + Ht + Sport, data = AIS)[, -1]
set.seed(42)
x1 <- rnorm(100, mean = 50, sd = 10)
x2 <- rnorm(100, mean = 1000, sd = 200)
y <- 3 + 0.5 * x1 - 0.002 * x2 + rnorm(100)
x <- as.matrix(data.frame(x1, x2))
library(glmnet)
model_std <- glmnet(x, y, standardize = TRUE)
coef(model_std, s = 0.1)  # choose lambda value
model_raw <- glmnet(x, y, standardize = FALSE)
coef(model_raw, s = 0.1)
library(glmnet)
# Prepare data
data(mtcars)
x <- as.matrix(mtcars[, -1])  # predictors (exclude mpg)
y <- mtcars$mpg              # response
model_std <- glmnet(x, y, alpha = 0.5)  # standardize = TRUE by default
coef(model_std, s = 0.1)  # extract coefficients at lambda = 0.1
model_raw <- glmnet(x, y, alpha = 0.5, standardize = FALSE)
coef(model_raw, s = 0.1)
plot(model_std, xvar = "lambda", label = TRUE)
library(ggplot2)
# 2-simplex is just a line segment from (0,1) to (1,0)
p1 <- seq(0, 1, by = 0.01)
p2 <- 1 - p1
df_2simplex <- data.frame(p1 = p1, p2 = p2)
ggplot(df_2simplex, aes(x = p1, y = p2)) +
geom_line(linewidth = 2, color = "blue") +
geom_point(data = data.frame(p1 = c(0, 1, 0.5),
p2 = c(1, 0, 0.5)),
aes(x = p1, y = p2), size = 4, color = "red") +
annotate("text", x = 0, y = 1, label = "(0, 1)", vjust = -1) +
annotate("text", x = 1, y = 0, label = "(1, 0)", vjust = -1) +
annotate("text", x = 0.5, y = 0.5, label = "(0.5, 0.5)", vjust = -1) +
labs(title = "2-Simplex (Line Segment)",
subtitle = "All points where p₁ + p₂ = 1",
x = "p₁", y = "p₂") +
coord_fixed() +
theme_minimal()
library(ggtern)  # For ternary plots
install.packages("ggtern")
library(ggtern)  # For ternary plots
# Some example points on the 3-simplex
examples <- data.frame(
label = c("Corner 1", "Corner 2", "Corner 3", "Center", "Edge midpoint"),
p1 = c(1, 0, 0, 1/3, 0.5),
p2 = c(0, 1, 0, 1/3, 0.5),
p3 = c(0, 0, 1, 1/3, 0)
)
print(examples)
#         label   p1   p2   p3
# 1    Corner 1  1.0  0.0  0.0
# 2    Corner 2  0.0  1.0  0.0
# 3    Corner 3  0.0  0.0  1.0
# 4      Center  0.33 0.33 0.33
# 5 Edge midpt  0.5  0.5  0.0
# Verify they sum to 1
rowSums(examples[, c("p1", "p2", "p3")])
# [1] 1 1 1 1 1 ✓
# Example microbiome sample
sample1 <- c(Bacteroides = 0.4,
Firmicutes = 0.35,
Proteobacteria = 0.15,
Actinobacteria = 0.1)
print(sample1)
sum(sample1)  # = 1 ✓
# This point lives on the 4-simplex!
# Two microbiome samples
sample_A <- c(0.50, 0.30, 0.20)  # Taxon 1, 2, 3
sample_B <- c(0.25, 0.15, 0.60)
# If Taxon 3 increases, others MUST decrease
# They're not independent!
# This creates spurious correlations
set.seed(42)
n <- 100
# Simulate compositional data
true_abundances <- matrix(runif(n * 3), ncol = 3)
compositions <- true_abundances / rowSums(true_abundances)
# Correlations look negative even if data is random!
cor(compositions)
#            [,1]       [,2]       [,3]
# [1,]  1.0000000 -0.6023959 -0.5985104
# [2,] -0.6023959  1.0000000 -0.4978827
# [3,] -0.5985104 -0.4978827  1.0000000
# These negative correlations are ARTIFACTS of the constraint!
setwd("~/Desktop/Pro_FN/PiZZ/Proteotoxicity/piz-shinylive-app")
shinylive::export(appdir = "app", destdir = "docs")
shinylive::export(appdir = "app", destdir = "./")
httpuv::runStaticServer(".")
shinylive::export(appdir = "app", destdir = "site")
httpuv::runStaticServer("site/")
shinylive::export(appdir = "app", destdir = "site")
httpuv::runStaticServer("site/")
